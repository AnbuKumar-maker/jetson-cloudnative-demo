BERT is a deep neural network used for natural language processing and understanding.  BERT means Bidirectional Encoder Representations from Transformers.  BERT is able to perform a variety of NLP tasks such as question answering, intent classification, sentiment analysis, paraphrasing, recommendations, and autocompletion.  BERT question answering works by providing a source passage paragraph at runtime which BERT can then answer questions about by selecting the most relevant text to the query from the passage.  The BERT model is accelerated on Jetson with GPU using TensorRT.
